@inproceedings{E17-3001,
 abstract = {In semantic parsing, natural language questions map to expressions in a meaning
representation language (MRL) over some fixed vocabulary of predicates. To do
this reliably, one must guarantee that for a wide class of natural language
questions (the so called semantically tractable questions), correct
interpretations are always in the mapped set of possibilities. In this
demonstration, we introduce the system COVER which significantly clarifies, revises and extends the basic notion of semantic tractability. COVER achieves
coverage of 89% while the earlier PRECISE system achieved coverage of 77% on
the well known GeoQuery corpus. Like PRECISE, COVER requires only a simple
domain lexicon and integrates off-the-shelf syntactic parsers. Beyond PRECISE, COVER also integrates off-the-shelf theorem provers to provide more accurate
results. COVER is written in Python and uses the NLTK.},
 address = {Valencia, Spain},
 author = {Minock, Michael},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {1--4},
 publisher = {Association for Computational Linguistics},
 title = {COVER: Covering the Semantically Tractable Questions},
 year = {2017}
}

@inproceedings{E17-3002,
 abstract = {Web debates play an important role in enabling broad participation of
constituencies in social, political and economic decision-taking. However, it
is challenging to organize, structure, and navigate a vast number of diverse
argumentations and comments collected from many participants over a long time
period. In this paper we demonstrate Common Round, a next generation platform
for large-scale web debates, which provides functions for eliciting the
semantic content and structures from the contributions of participants. In
particular, Common Round applies language technologies for the extraction of
semantic essence from textual input, aggregation of the formulated opinions and
arguments. The platform also provides a cross-lingual access to debates using
machine translation.},
 address = {Valencia, Spain},
 author = {Uszkoreit, Hans  and  Gabryszak, Aleksandra  and  Hennig, Leonhard  and  Steffen, J\"{o}rg  and  Ai, Renlong  and  Busemann, Stephan  and  Dehdari, Jon  and  van Genabith, Josef  and  Heigold, Georg  and  Rethmeier, Nils  and  Rubino, Raphael  and  Schmeier, Sven  and  Thomas, Philippe  and  Wang, He  and  Xu, Feiyu},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {5--8},
 publisher = {Association for Computational Linguistics},
 title = {Common Round: Application of Language Technologies to Large-Scale Web Debates},
 year = {2017}
}

@inproceedings{E17-3003,
 abstract = {The paper presents the Etymological DICtionary ediTOR (EDICTOR), a free, interactive, web-based tool designed to aid historical linguists in creating, editing, analysing, and publishing etymological datasets. The EDICTOR offers
interactive solutions for important tasks in historical linguistics, including
facilitated input and segmentation of phonetic transcriptions, quantitative and
qualitative analyses of phonetic and morphological data, enhanced interfaces
for cognate class assignment and multiple word alignment, and automated
evaluation of regular sound correspondences. As a web-based tool written in
JavaScript, the EDICTOR can be used in standard web browsers across all major
platforms.},
 address = {Valencia, Spain},
 author = {List, Johann-Mattis},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {9--12},
 publisher = {Association for Computational Linguistics},
 title = {A Web-Based Interactive Tool for Creating, Inspecting, Editing, and Publishing Etymological Datasets},
 year = {2017}
}

@inproceedings{E17-3004,
 abstract = {A frequent type of annotations in text corpora are labeled text segments.
General-purpose annotation tools tend to be overly comprehensive, often making
the annotation process slower and more error-prone. We present WAT-SL, a new
web-based tool that is dedicated to segment labeling and highly customizable to
the labeling task at hand. We outline its main features and exemplify how we
used it for a crowdsourced corpus with labeled argument units.},
 address = {Valencia, Spain},
 author = {Kiesel, Johannes  and  Wachsmuth, Henning  and  Al Khatib, Khalid  and  Stein, Benno},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {13--16},
 publisher = {Association for Computational Linguistics},
 title = {WAT-SL: A Customizable Web Annotation Tool for Segment Labeling},
 year = {2017}
}

@inproceedings{E17-3005,
 abstract = {R is a very powerful framework for statistical modeling. Thus, it is of high
importance to integrate R with state-of-the-art tools in NLP. In this paper, we
present the functionality and architecture of such an integration by means of
TextImager. We use the OpenCPU API to integrate R based on our own R-Server.
This allows for communicating with R-packages and combining them with
TextImager’s NLP-components.},
 address = {Valencia, Spain},
 author = {Uslu, Tolga  and  Hemati, Wahed  and  Mehler, Alexander  and  Baumartz, Daniel},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {17--20},
 publisher = {Association for Computational Linguistics},
 title = {TextImager as a Generic Interface to R},
 year = {2017}
}

@inproceedings{E17-3006,
 abstract = {Wikipedia offers researchers unique insights into the collaboration and
communication patterns of a large self-regulating community of editors. The
main medium of direct communication between editors of an article is the
article’s talk page. However, a talk page file is unstructured and therefore
difficult to analyse automatically. A few parsers exist that enable its
transformation into a structured data format. However, they are rarely open
source, support only a limited subset of the talk page syntax -- resulting in
the loss of content -- and usually support only one export format. Together
with this article we offer a very fast, lightweight, open source parser with
support for various output formats. In a preliminary evaluation it achieved a
high accuracy. The parser uses a grammar-based approach -- offering a
transparent implementation and easy extensibility.},
 address = {Valencia, Spain},
 author = {Cabrera, Benjamin  and  Steinert, Laura  and  Ross, Bj\"{o}rn},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {21--24},
 publisher = {Association for Computational Linguistics},
 title = {GraWiTas: a Grammar-based Wikipedia Talk Page Parser},
 year = {2017}
}

@inproceedings{E17-3007,
 abstract = {In the recent years, the amount of user generated contents shared on the Web
has significantly increased, especially in social media environment, e.g.
Twitter, Facebook, Google+. This large quantity of data has generated the need
of reactive and sophisticated systems for capturing and understanding the
underlying information enclosed in them.
In this paper we present TWINE, a real-time system for the big data analysis
and exploration of information extracted from Twitter streams.
The proposed system based on a Named Entity Recognition and Linking pipeline
and a multi-dimensional spatial geo-localization is managed by a scalable and
flexible architecture for an interactive visualization of micropost streams
insights.
The demo is available at http://twine-mind.cloudapp.net/streaming.},
 address = {Valencia, Spain},
 author = {Nozza, Debora  and  Ristagno, Fausto  and  Palmonari, Matteo  and  Fersini, Elisabetta  and  Manchanda, Pikakshi  and  Messina, Enza},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {25--28},
 publisher = {Association for Computational Linguistics},
 title = {TWINE: A real-time system for TWeet analysis via INformation Extraction},
 year = {2017}
}

@inproceedings{E17-3008,
 abstract = {We present Alto, a rapid prototyping tool for new grammar
formalisms. Alto implements generic but efficient algorithms for
parsing, translation, and training for a range of monolingual and
synchronous grammar formalisms. It can easily be extended to new
formalisms, which makes all of these algorithms immediately
available for the new formalism.},
 address = {Valencia, Spain},
 author = {Gontrum, Johannes  and  Groschwitz, Jonas  and  Koller, Alexander  and  Teichmann, Christoph},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {29--32},
 publisher = {Association for Computational Linguistics},
 title = {Alto: Rapid Prototyping for Parsing and Translation},
 year = {2017}
}

@inproceedings{E17-3009,
 abstract = {Voice enabled human computer interfaces (HCI) that integrate automatic speech
recognition, text-to-speech synthesis and natural language understanding have
become a commodity, introduced by the immersion of smart phones and other
gadgets in our daily lives. Smart assistants are able to respond to simple
queries (similar to text-based question-answering systems), perform simple
tasks (call a number, reject a call etc.) and help organizing appointments.
With this paper we introduce a newly created process automation platform that
enables the user to control applications and home appliances and to query the
system for information using a natural voice interface. We offer an overview of
the technologies that enabled us to construct our system and we present
different usage scenarios in home and office environments.},
 address = {Valencia, Spain},
 author = {Boro\c{s}, Tiberiu  and  Dumitrescu, Stefan Daniel  and  Pipa, Sonia},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {33--36},
 publisher = {Association for Computational Linguistics},
 title = {CASSANDRA: A multipurpose configurable voice-enabled human-computer-interface},
 year = {2017}
}

@inproceedings{E17-3010,
 abstract = {In this paper we present our automated fact checking system demonstration which
we developed in order to participate in the Fast and Furious Fact Check
challenge. We focused on simple numerical claims such as “population of
Germany in 2015 was 80 million” which comprised a quarter of the test
instances in the challenge, achieving 68% accuracy. Our system extends previous
work on semantic parsing and claim identification to handle temporal
expressions and knowledge bases consisting of multiple tables, while relying
solely on automatically generated training data. We demonstrate the extensible
nature of our system by evaluating it on relations used in previous work. We
make our system publicly available so that it can be used and extended by the
community.},
 address = {Valencia, Spain},
 author = {Thorne, James  and  Vlachos, Andreas},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {37--40},
 publisher = {Association for Computational Linguistics},
 title = {An Extensible Framework for Verification of Numerical Claims},
 year = {2017}
}

@inproceedings{E17-3011,
 abstract = {Distributing papers into sessions in scientific conferences is a task
consisting in grouping papers with common topics and considering the size
restrictions imposed by the conference schedule. This problem can be seen as a
semi-supervised clustering of scientific papers based on their features. This
paper presents a web tool called ADoCS that solves the problem of configuring
conference schedules by an automatic clustering of articles by similarity using
a new algorithm considering size constraints.},
 address = {Valencia, Spain},
 author = {Vallejo Huanga, Diego Fernando  and  Morillo Alc\'{i}var, Paulina Adriana  and  Ferri Ram\'{i}rez, Cesar},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {41--44},
 publisher = {Association for Computational Linguistics},
 title = {ADoCS: Automatic Designer of Conference Schedules},
 year = {2017}
}

@inproceedings{E17-3012,
 abstract = {This article describes a semantic system which is based on distributional
models
obtained from a chronologically structured language resource, namely Google
Books Syntactic Ngrams.The models were created using dependency-based contexts
and a strategy for reducing the vector space, which consists in selecting the
more informative and relevant word contexts. The system allowslinguists to
analize meaning change of Spanish words in the written language across time.},
 address = {Valencia, Spain},
 author = {Gamallo, Pablo  and  Rodr\'{i}guez-Torres, Iv\'{a}n  and  Garcia, Marcos},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {45--48},
 publisher = {Association for Computational Linguistics},
 title = {A Web Interface for Diachronic Semantic Search in Spanish},
 year = {2017}
}

@inproceedings{E17-3013,
 abstract = {This paper describes a web-based application to design and answer exercises for
language learning. It is available in Basque, Spanish, English, and French.
Based on open-source Natural Language Processing (NLP) technology such as word
embedding models and word sense disambiguation, the application enables users
to automatic create easily and in real time three types of exercises, namely, Fill-in-the-Gaps, Multiple Choice, and Shuffled Sentences questionnaires. These
are generated from texts of the users' own choice, so they can train their
language skills with content of their particular interest.},
 address = {Valencia, Spain},
 author = {Perez, Naiara  and  Cuadros, Montse},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {49--52},
 publisher = {Association for Computational Linguistics},
 title = {Multilingual CALL Framework for Automatic Language Exercise Generation from Free Text},
 year = {2017}
}

@inproceedings{E17-3014,
 abstract = {Understanding the social media audience is becoming increasingly important for
social media analysis. This paper presents an approach that detects various
audience attributes, including author location, demographics, behavior and
interests. It works both for a variety of social media sources and for multiple
languages. The approach has been implemented within IBM Watson Analytics for
Social Media and creates author profiles for more than 300 different analysis
domains every day.},
 address = {Valencia, Spain},
 author = {Henrich, Verena  and  Lang, Alexander},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {53--56},
 publisher = {Association for Computational Linguistics},
 title = {Audience Segmentation in Social Media},
 year = {2017}
}

@inproceedings{E17-3015,
 abstract = {This article describes an automatic system for writing specialized texts in
Spanish. The arText prototype is a free online text editor that includes
different types of linguistic information. It is designed for a variety of end
users and domains, including specialists and university students working in the
fields of medicine and tourism, and laypersons writing to the public
administration. ArText provides guidance on how to structure a text, prompts
users to include all necessary contents in each section, and detects lexical
and discourse problems in the text.},
 address = {Valencia, Spain},
 author = {da Cunha, Iria  and  Montan\'{e}, M. Amor  and  Hysa, Luis},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {57--60},
 publisher = {Association for Computational Linguistics},
 title = {The arText prototype: An automatic system for writing specialized texts},
 year = {2017}
}

@inproceedings{E17-3016,
 abstract = {This paper presents QCRI’s Arabic-to-English live speech translation system.
It features modern web technologies to capture live audio, and broadcasts
Arabic transcriptions and English translations simultaneously. Our Kaldi-based
ASR system uses the Time Delay Neural Network (TDNN) architecture, while our
Machine Translation (MT) system uses both phrase-based and neural frameworks.
Although our neural MT system is slower than the phrase-based system, it
produces significantly better translations and is memory efficient. The demo is
available at https://st.qcri.org/demos/livetranslation.},
 address = {Valencia, Spain},
 author = {Dalvi, Fahim  and  Zhang, Yifan  and  Khurana, Sameer  and  Durrani, Nadir  and  Sajjad, Hassan  and  Abdelali, Ahmed  and  Mubarak, Hamdy  and  Ali, Ahmed  and  Vogel, Stephan},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {61--64},
 publisher = {Association for Computational Linguistics},
 title = {QCRI Live Speech Translation System},
 year = {2017}
}

@inproceedings{E17-3017,
 abstract = {We present Nematus, a toolkit for Neural Machine Translation. The toolkit
prioritizes high translation accuracy, usability, and extensibility. Nematus
has been used to build top-performing submissions to shared translation tasks
at WMT and IWSLT, and has been used to train systems for production
environments.},
 address = {Valencia, Spain},
 author = {Sennrich, Rico  and  Firat, Orhan  and  Cho, Kyunghyun  and  Birch, Alexandra  and  Haddow, Barry  and  Hitschler, Julian  and  Junczys-Dowmunt, Marcin  and  L\"{a}ubli, Samuel  and  Miceli Barone, Antonio Valerio  and  Mokry, Jozef  and  Nadejde, Maria},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {65--68},
 publisher = {Association for Computational Linguistics},
 title = {Nematus: a Toolkit for Neural Machine Translation},
 year = {2017}
}

@inproceedings{E17-3018,
 abstract = {This paper describes an application system aimed to help lexicographers in the
extraction of example sentences for a given headword based on its different
senses. The tool uses classification and clustering methods and incorporates
user feedback to refine its results.},
 address = {Valencia, Spain},
 author = {Boullosa, Beto  and  Eckart de Castilho, Richard  and  Geyken, Alexander  and  Lemnitzer, Lothar  and  Gurevych, Iryna},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {69--72},
 publisher = {Association for Computational Linguistics},
 title = {A tool for extracting sense-disambiguated example sentences through user feedback},
 year = {2017}
}

@inproceedings{E17-3019,
 abstract = {Lingmotif is a lexicon-based, linguistically-motivated, user-friendly, GUI-enabled, multi-platform, Sentiment Analysis desktop application. Lingmotif
can perform SA on any type of input texts, regardless of their length and
topic. The analysis is based on the identification of sentiment-laden words and
phrases contained in the application's rich core lexicons, and employs context
rules to account for sentiment shifters. It offers easy-to-interpret visual
representations of quantitative data (text polarity, sentiment intensity, sentiment profile), as well as a detailed, qualitative analysis of the text in
terms of its sentiment. Lingmotif can also take user-provided plugin lexicons
in order to account for domain-specific sentiment expression. Lingmotif
currently analyzes English and Spanish texts.},
 address = {Valencia, Spain},
 author = {Moreno-Ortiz, Antonio},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {73--76},
 publisher = {Association for Computational Linguistics},
 title = {Lingmotif: Sentiment Analysis for the Digital Humanities},
 year = {2017}
}

@inproceedings{E17-3020,
 abstract = {We present RAMBLE ON, an application integrating a pipeline for frame-based
information extraction and an interface to track and display movement
trajectories. The code of the extraction pipeline and a navigator are freely
available; moreover we display in a demonstrator the outcome of a case study
carried out on trajectories of notable persons of the XX Century.},
 address = {Valencia, Spain},
 author = {Menini, Stefano  and  Sprugnoli, Rachele  and  Moretti, Giovanni  and  Bignotti, Enrico  and  Tonelli, Sara  and  Lepri, Bruno},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {77--80},
 publisher = {Association for Computational Linguistics},
 title = {RAMBLE ON: Tracing Movements of Popular Historical Figures},
 year = {2017}
}

@inproceedings{E17-3021,
 abstract = {This paper presents Autobank, a prototype tool for constructing a wide-coverage
Minimalist Grammar (MG) (Stabler 1997), and semi-automatically converting the
Penn Treebank (PTB) into a deep Minimalist treebank.  The front end of the tool
is a graphical user interface which facilitates the rapid development of a seed
set of MG trees via manual reannotation of PTB preterminals with MG lexical
categories. The system then extracts various dependency mappings between the
source and target trees, and uses these in concert with a non-statistical MG
parser to automatically reannotate the rest of the corpus.  Autobank thus
enables deep treebank conversions (and subsequent modifications) without the
need for complex transduction algorithms accompanied by cascades of ad hoc
rules; instead, the locus of human effort falls directly on the task of grammar
construction itself.},
 address = {Valencia, Spain},
 author = {Torr, John},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {81--86},
 publisher = {Association for Computational Linguistics},
 title = {Autobank: a semi-automatic annotation tool for developing deep Minimalist Grammar treebanks},
 year = {2017}
}

@inproceedings{E17-3022,
 abstract = {We build a chat bot with iterative content exploration that leads a user
through a personalized knowledge acquisition session. The chat bot is designed
as an automated customer support or product recommendation agent assisting a
user in learning product features, product usability, suitability, troubleshooting and other related tasks. To control the user navigation through
con- tent, we extend the notion of a linguistic discourse tree (DT) towards a
set of documents with multiple sections covering a topic. For a given
paragraph, a DT is built by DT parsers. We then combine DTs for the paragraphs
of documents to form what we call extended DT, which is a basis for interactive
content exploration facilitated by the chat bot. To provide cohesive answers, we use a measure of rhetoric agreement between a question and an answer by tree
kernel learning of their DTs.},
 address = {Valencia, Spain},
 author = {Galitsky, Boris  and  Ilvovsky, Dmitry},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {87--90},
 publisher = {Association for Computational Linguistics},
 title = {Chatbot with a Discourse Structure-Driven Dialogue Management},
 year = {2017}
}

@inproceedings{E17-3023,
 abstract = {We report on a demonstration system for text mining of literature in marine
science and related disciplines. It automatically  extracts variables ("CO2")
involved in events of change/increase/decrease ("increasing CO2"), as well as
co-occurrence and causal relations among these events ("increasing CO2 causes a
decrease in pH in seawater"), resulting in a big knowledge graph. A web-based
graphical user interface targeted at marine scientists facilitates searching, browsing and visualising events and their relations in an interactive way.},
 address = {Valencia, Spain},
 author = {Marsi, Erwin  and  Pinar {\O}zturk, Pinar  and  V. Ardelan, Murat},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {91--94},
 publisher = {Association for Computational Linguistics},
 title = {Marine Variable Linker: Exploring Relations between Changing Variables in Marine Science Literature},
 year = {2017}
}

@inproceedings{E17-3024,
 abstract = {This paper details a software designed to track neologisms in seven languages
through newspapers monitor corpora. The platform combines state-of-the-art
processes to track linguistic changes and a web platform for linguists to
create and manage their corpora, accept or reject automatically identified
neologisms, describe linguistically the accepted neologisms and follow their
lifecycle on the monitor corpora. In the following, after a short
state-of-the-art in Neologism Retrieval, Analysis and Life-tracking, we
describe the overall architecture of the system. The platform can be freely
browsed at www.neoveille.org where detailed presentation is given.  Access to
the editing modules is available upon request.},
 address = {Valencia, Spain},
 author = {Cartier, Emmanuel},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {95--98},
 publisher = {Association for Computational Linguistics},
 title = {Neoveille, a Web Platform for Neologism Tracking},
 year = {2017}
}

@inproceedings{E17-3025,
 abstract = {In this demo we present WebVectors, a free and open-source toolkit helping to
deploy web services which demonstrate and visualize distributional semantic
models (widely known as word embeddings).
WebVectors can be useful in a very common situation when one has trained a
distributional semantics model for one's particular corpus or language (tools
for this are now widespread and simple to use), but then there is a need to
demonstrate the results to general public over the Web.
We show its abilities on the example of the living web services featuring
distributional models for English, Norwegian and Russian.},
 address = {Valencia, Spain},
 author = {Kutuzov, Andrey  and  Kuzmenko, Elizaveta},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {99--103},
 publisher = {Association for Computational Linguistics},
 title = {Building Web-Interfaces for Vector Semantic Models with the WebVectors Toolkit},
 year = {2017}
}

@inproceedings{E17-3026,
 abstract = {Event Schema Induction is the task of learning a representation of events
(e.g., bombing) and the roles involved in them (e.g, victim and perpetrator).
This paper presents InToEventS, an interactive tool for learning these schemas.
InToEventS allows users to explore a corpus and discover which kind of events
are present. We show how users can create useful event schemas using two
interactive clustering steps.},
 address = {Valencia, Spain},
 author = {Ferrero, Germ\'{a}n  and  Primadhanty, Audi  and  Quattoni, Ariadna},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {104--107},
 publisher = {Association for Computational Linguistics},
 title = {InToEventS: An Interactive Toolkit for Discovering and Building Event Schemas},
 year = {2017}
}

@inproceedings{E17-3027,
 abstract = {Collocation and idiom extraction are well-known challenges with many potential
applications in Natural Language Processing (NLP). Our experimental, open-source software system, called ICE, is a python package for flexibly
extracting collocations and idioms, currently in English. It also has a
competitive POS tagger that can be used alone or as part of collocation/idiom
extraction. ICE is available free of cost for research and educational uses in
two user-friendly formats. This paper gives an overview of ICE and its
performance, and briefly describes the research underlying the extraction
algorithms.},
 address = {Valencia, Spain},
 author = {vuppuluri, vasanthi  and  Baki, Shahryar  and  Nguyen, An  and  Verma, Rakesh},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {108--111},
 publisher = {Association for Computational Linguistics},
 title = {ICE: Idiom and Collocation Extractor for Research and Education},
 year = {2017}
}

@inproceedings{E17-3028,
 abstract = {We propose a novel embedding model that represents relationships among several
elements in bibliographic information with high representation ability and
flexibility. Based on this model, we present a novel search system that shows
the relationships among the elements in the ACL Anthology Reference Corpus. The
evaluation results show that our model can achieve a high prediction ability
and produce reasonable search results.},
 address = {Valencia, Spain},
 author = {Yoneda, Takuma  and  Mori, Koki  and  Miwa, Makoto  and  Sasaki, Yutaka},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {112--115},
 publisher = {Association for Computational Linguistics},
 title = {Bib2vec: Embedding-based Search System for Bibliographic Information},
 year = {2017}
}

@inproceedings{E17-3029,
 abstract = {We present the first prototype of the SUMMA Platform: an integrated platform
for multilingual media monitoring. The platform contains a rich suite of
low-level and high-level natural language processing technologies: automatic
speech recognition of broadcast media, machine translation, automated tagging
and classification of named entities, semantic parsing to detect relationships
between entities, and automatic construction / augmentation of factual
knowledge bases. Implemented on the Docker platform, it can easily be deployed, customised, and scaled to large volumes of incoming media streams.},
 address = {Valencia, Spain},
 author = {Liepins, Renars  and  Germann, Ulrich  and  Barzdins, Guntis  and  Birch, Alexandra  and  Renals, Steve  and  Weber, Susanne  and  van der Kreeft, Peggy  and  Bourlard, Herve  and  Prieto, Jo\~{a}o  and  Klejch, Ondrej  and  Bell, Peter  and  Lazaridis, Alexandros  and  Mendes, Alfonso  and  Riedel, Sebastian  and  Almeida, Mariana S. C.  and  Balage, Pedro  and  Cohen, Shay B.  and  Dwojak, Tomasz  and  Garner, Philip N.  and  Giefer, Andreas  and  Junczys-Dowmunt, Marcin  and  Imran, Hina  and  Nogueira, David  and  Ali, Ahmed  and  Miranda, Sebasti\~{a}o  and  Popescu-Belis, Andrei  and  Miculicich Werlen, Lesly  and  Papasarantopoulos, Nikos  and  Obamuyide, Abiola  and  Jones, Clive  and  Dalvi, Fahim  and  Vlachos, Andreas  and  Wang, Yang  and  Tong, Sibo  and  Sennrich, Rico  and  Pappas, Nikolaos  and  Narayan, Shashi  and  Damonte, Marco  and  Durrani, Nadir  and  Khurana, Sameer  and  Abdelali, Ahmed  and  Sajjad, Hassan  and  Vogel, Stephan  and  Sheppey, David  and  Hernon, Chris  and  Mitchell, Jeff},
 booktitle = {Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
 month = {April},
 pages = {116--119},
 publisher = {Association for Computational Linguistics},
 title = {The SUMMA Platform Prototype},
 year = {2017}
}

