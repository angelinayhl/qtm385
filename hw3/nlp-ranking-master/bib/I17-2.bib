@inproceedings{I17-2001,
 abstract = {This paper proposes a new attention
mechanism for neural machine translation
(NMT) based on convolutional neural
networks (CNNs), which is inspired
by the CKY algorithm. The proposed attention
represents every possible combination
of source words (e.g., phrases and
structures) through CNNs, which imitates
the CKY table in the algorithm. NMT, incorporating the proposed attention, decodes
a target sentence on the basis of
the attention scores of the hidden states
of CNNs. The proposed attention enables
NMT to capture alignments from underlying
structures of a source sentence
without sentence parsing. The evaluations
on the Asian Scientific Paper Excerpt
Corpus (ASPEC) English-Japanese translation
task show that the proposed attention
gains 0.66 points in BLEU.},
 address = {Taipei, Taiwan},
 author = {Watanabe, Taiki  and  Tamura, Akihiro  and  Ninomiya, Takashi},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {1--6},
 publisher = {Asian Federation of Natural Language Processing},
 title = {CKY-based Convolutional Attention for Neural Machine Translation},
 year = {2017}
}

@inproceedings{I17-2002,
 abstract = {The sequence-to-sequence (Seq2Seq) model has been successfully applied to
machine translation (MT). Recently, MT performances were improved by
incorporating supervised attention into the model. In this paper, we introduce
supervised attention to constituency parsing that can be regarded as another
translation task. Evaluation results on the PTB corpus showed that the
bracketing F-measure was improved by supervised attention.},
 address = {Taipei, Taiwan},
 author = {Kamigaito, Hidetaka  and  Hayashi, Katsuhiko  and  Hirao, Tsutomu  and  Takamura, Hiroya  and  Okumura, Manabu  and  Nagata, Masaaki},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {7--12},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Supervised Attention for Sequence-to-Sequence Constituency Parsing},
 year = {2017}
}

@inproceedings{I17-2003,
 abstract = {Our paper addresses the problem of annotation  projection  for

semantic
role
labeling for resource-poor languages using supervised annotations from a
resource-rich language through                                      parallel

data.  We
propose
a transfer
method that  employs  information from source and target syntactic dependencies
as  well as  word  alignment density to improve the quality of an iterative
bootstrapping  method.                                       Our
experiments yield a
3.5
absolute
labeled
F-score
improvement  over  a  standard annotation projection method.},
 address = {Taipei, Taiwan},
 author = {Aminian, Maryam  and  Rasooli, Mohammad Sadegh  and  Diab, Mona},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {13--19},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Transferring Semantic Roles Using Translation and Syntactic Information},
 year = {2017}
}

@inproceedings{I17-2004,
 abstract = {Domain adaptation is a major challenge for neural machine translation (NMT).
Given unknown words or new domains, NMT systems tend to generate fluent
translations at the expense of adequacy. We present a stack-based lattice
search algorithm for NMT and show that constraining its search space with
lattices generated by phrase-based machine translation (PBMT) improves
robustness. We report consistent BLEU score gains across four diverse domain
adaptation tasks involving medical, IT, Koran, or subtitles texts.},
 address = {Taipei, Taiwan},
 author = {Khayrallah, Huda  and  Kumar, Gaurav  and  Duh, Kevin  and  Post, Matt  and  Koehn, Philipp},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {20--25},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Neural Lattice Search for Domain Adaptation in Machine Translation},
 year = {2017}
}

@inproceedings{I17-2005,
 abstract = {This paper tackles a problem of analyzing the well-formedness of syllables in
Japanese Sign Language (JSL). We formulate the problem as a classification
problem that classifies syllables into well-formed or ill-formed. We build a
data set that contains hand-coded syllables and their well-formedness. We
define a fine-grained feature set based on the hand-coded syllables and train a
logistic regression classifier on labeled syllables, expecting to find the
discriminative features from the trained classifier. We also perform pseudo
active learning to investigate the applicability of active learning in
analyzing syllables. In the experiments, the best classifier with our
combinatorial features achieved the accuracy of 87.0%. The pseudo active
learning is also shown to be effective showing that it could reduce about 84%
of training instances to achieve the accuracy of 82.0% when compared to the
model without active learning.},
 address = {Taipei, Taiwan},
 author = {Yawata, Satoshi  and  Miwa, Makoto  and  Sasaki, Yutaka  and  Hara, Daisuke},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {26--30},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Analyzing Well-Formedness of Syllables in Japanese Sign Language},
 year = {2017}
}

@inproceedings{I17-2006,
 abstract = {Word embeddings are a relatively new addition to the modern NLP researcher's
toolkit. However, unlike other tools, word embeddings are used in a black box
manner. There are very few studies regarding various hyperparameters. One such
hyperparameter is the dimension of word embeddings. They are rather decided
based on a rule of thumb: in the range 50 to 300. In this paper, we show that
the dimension should instead be chosen based on corpus statistics. More
specifically, we show that the number of pairwise equidistant words of the
corpus vocabulary (as defined by some distance/similarity metric) gives a lower
bound  on the the number of dimensions , and going below this bound results in
degradation of quality of learned word embeddings. Through our evaluations on
standard word embedding evaluation tasks, we show that for dimensions higher
than or equal to the bound, we get better results as compared to the ones below
it.},
 address = {Taipei, Taiwan},
 author = {Patel, Kevin  and  Bhattacharyya, Pushpak},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {31--36},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Towards Lower Bounds on Number of Dimensions for Word Embeddings},
 year = {2017}
}

@inproceedings{I17-2007,
 abstract = {This paper presents an approach to the task of predicting an event description
from a preceding sentence in a text. Our approach explores sequence-to-sequence
learning using a bidirectional multi-layer recurrent neural network. Our
approach substantially outperforms previous work in terms of the BLEU score on
two datasets derived from WikiHow and DeScript respectively. Since the BLEU
score is not easy to interpret as a measure of event prediction, we complement
our study with a second evaluation that exploits the rich linguistic annotation
of gold paraphrase sets of events.},
 address = {Taipei, Taiwan},
 author = {Nguyen, Dai Quoc  and  Nguyen, Dat Quoc  and  Chu, Cuong Xuan  and  Thater, Stefan  and  Pinkal, Manfred},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {37--42},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Sequence to Sequence Learning for Event Prediction},
 year = {2017}
}

@inproceedings{I17-2008,
 abstract = {This paper proposes a reinforcing method that refines the output layers of
existing Recurrent Neural Network (RNN) language models.
We refer to our proposed method as Input-to-Output Gate (IOG).
IOG has an extremely simple structure, and thus, can be easily combined with
any RNN language models.
Our experiments on the Penn Treebank and WikiText-2 datasets demonstrate that
IOG consistently boosts the performance of several different types of current
topline RNN language models.},
 address = {Taipei, Taiwan},
 author = {Takase, Sho  and  Suzuki, Jun  and  Nagata, Masaaki},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {43--48},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Input-to-Output Gate to Improve RNN Language Models},
 year = {2017}
}

@inproceedings{I17-2009,
 abstract = {Mobile devices use language models to suggest words and phrases for use in text
entry. Traditional language models are based on contextual word frequency in a
static corpus of text. However, certain types of phrases, when offered to
writers as suggestions, may be systematically chosen more often than their
frequency would predict. In this paper, we propose the task of generating
suggestions that writers accept, a related but distinct task to making accurate
predictions. Although this task is fundamentally interactive, we propose a
counterfactual setting that permits offline training and evaluation. We find
that even a simple language model can capture text characteristics that improve
acceptability.},
 address = {Taipei, Taiwan},
 author = {Arnold, Kenneth  and  Chang, Kai-Wei  and  Kalai, Adam},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {49--54},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Counterfactual Language Model Adaptation for Suggesting Phrases},
 year = {2017}
}

@inproceedings{I17-2010,
 abstract = {Multi-task learning (MTL) has recently contributed to learning better
representations in service of various NLP tasks. MTL aims at improving the
performance of a primary task by jointly training on a secondary task. This
paper introduces automated tasks, which exploit the sequential nature of the
input data, as secondary tasks in an MTL model. We explore next word
prediction, next character prediction, and missing word completion as potential
automated tasks. Our results show that training on a primary task in parallel
with a secondary automated task improves both the convergence speed and
accuracy for the primary task. We suggest two methods for augmenting an
existing network with automated tasks and establish better performance in topic
prediction, sentiment analysis, and hashtag recommendation. Finally, we show
that the MTL models can perform well on datasets that are small and colloquial
by nature.},
 address = {Taipei, Taiwan},
 author = {Liang, Davis  and  Shu, Yan},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {55--60},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Deep Automated Multi-task Learning},
 year = {2017}
}

@inproceedings{I17-2011,
 abstract = {In Multilabel Learning (MLL) each training instance is associated with a set of
labels and the task is to learn a function that maps an unseen instance to its
corresponding label set. In this paper, we present a suite of -- MLL algorithm
independent -- post-processing techniques that utilize the conditional and
directional label-dependences in order to make the predictions from any MLL
approach more coherent and precise. We solve  constraint optimization problem
over the output produced by any MLL approach and the result is a refined
version of the input predicted label set. Using proposed techniques, we show
absolute improvement of 3% on English News and 10% on Chinese E-commerce
datasets for P$@$K metric.},
 address = {Taipei, Taiwan},
 author = {Soni, Akshay  and  Pappu, Aasish  and  Ni, Jerry Chia-mau  and  Chevalier, Troy},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {61--66},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Post-Processing Techniques for Improving Predictions of Multilabel Learning Approaches},
 year = {2017}
}

@inproceedings{I17-2012,
 abstract = {Non-contiguous word sequences are widely known to be important in modelling
natural language. However they not explicitly encoded in common text
representations. In this work we propose a model for text processing using
string kernels, capable of flexibly representing non-contiguous sequences.
Specifically, we derive a vectorised version of the string kernel algorithm and
their gradients, allowing efficient hyperparameter optimisation as part of a
Gaussian Process framework. Experiments on synthetic data and text regression
for emotion analysis show the promise of this technique.},
 address = {Taipei, Taiwan},
 author = {Beck, Daniel  and  Cohn, Trevor},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {67--73},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Learning Kernels over Strings using Gaussian Processes},
 year = {2017}
}

@inproceedings{I17-2013,
 abstract = {Word segmentation is crucial in natural language processing tasks for
unsegmented languages. In Japanese, many out-of-vocabulary words appear in the
phonetic syllabary katakana, making segmentation more difficult due to the lack
of clues found in mixed script settings. In this paper, we propose a
straightforward approach based on a variant of tf-idf and apply it to the
problem of word segmentation in Japanese. Even though our method uses only an
unlabeled corpus, experimental results show that it achieves performance
comparable to existing methods that use manually labeled corpora. Furthermore, it improves performance of simple word segmentation models trained on a
manually labeled corpus.},
 address = {Taipei, Taiwan},
 author = {Fujinuma, Yoshinari  and  Grissom II, Alvin},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {74--79},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Substring Frequency Features for Segmentation of Japanese Katakana Words with Unlabeled Corpora},
 year = {2017}
}

@inproceedings{I17-2014,
 abstract = {Part-of-speech (POS) tagging and named entity recognition (NER) are crucial
steps in natural language processing. In addition, the difficulty of word
segmentation places additional burden on those who intend to deal with
languages such as Chinese, and pipelined systems often suffer from error
propagation. This work proposes an end-to-end model using character-based
recurrent neural network (RNN) to jointly accomplish segmentation, POS tagging
and NER of a Chinese sentence. Experiments on previous word segmentation
and NER datasets show that a single model with the proposed architecture is
comparable to those trained specifically for each task, and outperforms
freely-available softwares.  Moreover, we provide a web-based interface for the
public to easily access this resource.},
 address = {Taipei, Taiwan},
 author = {Hsieh, Yu-Lun  and  Chang, Yung-Chun  and  Huang, Yi-Jie  and  Yeh, Shu-Hao  and  Chen, Chun-Hung  and  Hsu, Wen-Lian},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {80--85},
 publisher = {Asian Federation of Natural Language Processing},
 title = {MONPA: Multi-objective Named-entity and Part-of-speech Annotator for Chinese using Recurrent Neural Network},
 year = {2017}
}

@inproceedings{I17-2015,
 abstract = {We extensively analyse the correlations and drawbacks of conventionally
employed evaluation metrics for word segmentation. Unlike in standard
information retrieval, precision favours under-splitting systems and therefore
can be misleading in word segmentation. Overall, based on both theoretical and
experimental analysis, we propose that precision should be excluded from the
standard evaluation metrics and that the evaluation score obtained by using
only recall is sufficient and better correlated with the performance of word
segmentation systems.},
 address = {Taipei, Taiwan},
 author = {Shao, Yan  and  Hardmeier, Christian  and  Nivre, Joakim},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {86--90},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Recall is the Proper Evaluation Metric for Word Segmentation},
 year = {2017}
}

@inproceedings{I17-2016,
 abstract = {Low-resource named entity recognition is still an open problem in NLP. Most
state-of-the-art systems require tens of thousands of annotated sentences in
order to obtain high performance. However, for most of the world's languages it
is unfeasible to obtain such annotation. In this paper, we present a transfer
learning scheme, whereby we train character-level neural CRFs to predict named
entities for both high-resource languages and low-resource languages jointly.
Learning character representations for multiple related languages allows
knowledge transfer from the high-resource languages to the low-resource ones, improving F1 by up to 9.8 points.},
 address = {Taipei, Taiwan},
 author = {Cotterell, Ryan  and  Duh, Kevin},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {91--96},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Low-Resource Named Entity Recognition with Cross-lingual, Character-Level Neural Conditional Random Fields},
 year = {2017}
}

@inproceedings{I17-2017,
 abstract = {We present Segment-level Neural CRF, which combines neural networks with a linear chain CRF
for segment-level sequence modeling tasks
such as named entity recognition (NER) and syntactic chunking.
Our segment-level CRF can consider higher-order label dependencies compared
with conventional word-level CRF.
Since it is difficult to consider all possible variable length segments, our method uses segment lattice constructed from the word-level tagging model
to reduce the search space.
Performing experiments on NER and chunking, we demonstrate that our method
outperforms conventional word-level CRF with neural networks.},
 address = {Taipei, Taiwan},
 author = {Sato, Motoki  and  Shindo, Hiroyuki  and  Yamada, Ikuya  and  Matsumoto, Yuji},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {97--102},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Segment-Level Neural Conditional Random Fields for Named Entity Recognition},
 year = {2017}
}

@inproceedings{I17-2018,
 abstract = {We present and take advantage of the inherent visualizability properties of
words in visual corpora (the textual components of vision-language datasets) to
compute concreteness scores for words. Our simple method does not require
hand-annotated concreteness score lists for training, and yields
state-of-the-art results when evaluated against concreteness scores lists and
previously derived scores, as well as when used for metaphor detection.},
 address = {Taipei, Taiwan},
 author = {Kehat, Gitit  and  Pustejovsky, James},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {103--108},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Integrating Vision and Language Datasets to Measure Word Concreteness},
 year = {2017}
}

@inproceedings{I17-2019,
 abstract = {This paper examines the usefulness of semantic features based on word
alignments for estimating the quality of text simplification.
Specifically, we introduce seven types of alignment-based features computed on
the basis of word embeddings and paraphrase lexicons.
Through an empirical experiment using the QATS dataset, we confirm that we can
achieve the state-of-the-art performance only with these features.},
 address = {Taipei, Taiwan},
 author = {Kajiwara, Tomoyuki  and  Fujita, Atsushi},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {109--115},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Semantic Features Based on Word Alignments for Estimating Quality of Text Simplification},
 year = {2017}
}

@inproceedings{I17-2020,
 abstract = {Word embeddings learned from text cor-
pus can be improved by injecting knowl-
edge from external resources, while at
the same time also specializing them for
similarity or relatedness. These knowl-
edge resources (like WordNet, Paraphrase
Database) may not exist for all languages.
In this work we introduce a method to in-
ject word embeddings of a language with
knowledge resource of another language
by leveraging bilingual embeddings. First
we improve word embeddings of Ger-
man, Italian, French and Spanish using re-
sources of English and test them on variety
of word similarity tasks. Then we demon-
strate the utility of our method by creating
improved embeddings for Urdu and Tel-
ugu languages using Hindi WordNet, beat-
ing the previously established baseline for
Urdu.},
 address = {Taipei, Taiwan},
 author = {Pandey, Prakhar  and  Pudi, Vikram  and  Shrivastava, Manish},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {116--121},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Injecting Word Embeddings with Another Language's Resource : An Application of Bilingual Embeddings},
 year = {2017}
}

@inproceedings{I17-2021,
 abstract = {Speech is a natural channel for human-computer interaction in robotics and
consumer applications.
Natural language understanding pipelines that start with speech can have
trouble recovering from speech recognition errors.
Black-box automatic speech recognition (ASR) systems, built for general purpose
use, are unable to take advantage of in-domain language models that could
otherwise ameliorate these errors.
In this work, we present a method for re-ranking black-box ASR hypotheses using
an in-domain language model and semantic parser trained for a particular task.
Our re-ranking method significantly improves both transcription accuracy and
semantic understanding over a state-of-the-art ASR's vanilla output.},
 address = {Taipei, Taiwan},
 author = {Corona, Rodolfo  and  Thomason, Jesse  and  Mooney, Raymond},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {122--127},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Improving Black-box Speech Recognition using Semantic Parsing},
 year = {2017}
}

@inproceedings{I17-2022,
 abstract = {The research trend in Japanese predicate-argument structure (PAS) analysis is
shifting from pointwise prediction models with local features to global models
designed to search for globally optimal solutions. However, the existing global
models tend to employ only relatively simple local features; therefore, the
overall performance gains are rather limited. The importance of designing a
local model is demonstrated in this study by showing that the performance of a
sophisticated local model can be considerably improved with recent feature
embedding methods and a feature combination learning based on a neural network, outperforming the state-of-the-art global models in F1 on a common benchmark
dataset.},
 address = {Taipei, Taiwan},
 author = {Matsubayashi, Yuichiroh  and  Inui, Kentaro},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {128--133},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Revisiting the Design Issues of Local Models for Japanese Predicate-Argument Structure Analysis},
 year = {2017}
}

@inproceedings{I17-2023,
 abstract = {When giving descriptions, speakers often signify object shape or size with hand
gestures. Such so-called âiconicâ gestures represent their meaning through
their relevance to referents in the verbal content, rather than having a
conventional form. The gesture form on its own is often ambiguous, and the
aspect of the referent that it highlights is constrained by what the language
makes salient. We show how the verbal content guides gesture interpretation
through a computational model that frames the task as a multi-label
classification task that maps multimodal utterances to semantic categories, using annotated human-human data.},
 address = {Taipei, Taiwan},
 author = {Han, Ting  and  Hough, Julian  and  Schlangen, David},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {134--139},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Natural Language Informs the Interpretation of Iconic Gestures: A Computational Approach},
 year = {2017}
}

@inproceedings{I17-2024,
 abstract = {Emotion Analysis is the task of modelling latent emotions present in natural
language. Labelled datasets for this task are scarce so learning good input
text representations is not trivial. Using averaged word embeddings is a simple
way to leverage unlabelled corpora to build text representations but this
approach can be prone to noise either coming from the embedding themselves or
the averaging procedure. In this paper we propose a model for Emotion Analysis
using Gaussian Processes and kernels that are better suitable for functions
that exhibit noisy behaviour. Empirical evaluations in a emotion prediction
task show that our model outperforms commonly used baselines for regression.},
 address = {Taipei, Taiwan},
 author = {Beck, Daniel},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {140--145},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Modelling Representation Noise in Emotion Analysis using Gaussian Processes},
 year = {2017}
}

@inproceedings{I17-2025,
 abstract = {In this paper, we investigate the effectiveness of different affective lexicons
through sentiment analysis of phrases. We examine how phrases can be
represented through manually prepared lexicons, extended lexicons using
computational methods, or word embedding. Comparative studies clearly show that
word embedding using unsupervised distributional method outperforms manually
prepared lexicons no matter what affective models are used in the lexicons. Our
conclusion is that although different affective lexicons are cognitively backed
by theories, they do not show any advantage over the automatically obtained
word embedding.},
 address = {Taipei, Taiwan},
 author = {Li, Minglei  and  Lu, Qin  and  Long, Yunfei},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {146--150},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Are Manually Prepared Affective Lexicons Really Useful for Sentiment Analysis},
 year = {2017}
}

@inproceedings{I17-2026,
 abstract = {Online reviews are valuable resources not only for consumers to make decisions
before purchase, but also for providers to get feedbacks for their services or
commodities. In Aspect Based Sentiment Analysis (ABSA), it is critical to
identify aspect categories and extract aspect terms from the sentences of
user-generated reviews.
However, the two tasks are often treated independently, even though they are
closely related. Intuitively, the learned knowledge of one task should inform
the other learning task. In this paper, we propose a multi-task learning model
based on neural networks to solve them together. We demonstrate the improved
performance of our multi-task learning model over the models trained separately
on three public dataset released by SemEval workshops.},
 address = {Taipei, Taiwan},
 author = {Xue, Wei  and  Zhou, Wubai  and  Li, Tao  and  Wang, Qing},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {151--156},
 publisher = {Asian Federation of Natural Language Processing},
 title = {MTNA: A Neural Multi-task Model for Aspect Category Classification and Aspect Term Extraction On Restaurant Reviews},
 year = {2017}
}

@inproceedings{I17-2027,
 abstract = {Humans process language word by word
and construct partial linguistic structures
on the fly before the end of the sentence
is perceived. Inspired by this cognitive
ability, incremental algorithms for natural
language processing tasks have been proposed
and demonstrated promising performance.
For discourse relation (DR) parsing, however, it is not yet clear to what
extent humans can recognize DRs incrementally, because the latent ânodesâ of
discourse structure can span clauses and
sentences. To answer this question, this
work investigates incrementality in discourse
processing based on a corpus annotated
with DR signals. We find that
DRs are dominantly signaled at the boundary
between the two constituent discourse
units. The findings complement existing
psycholinguistic theories on expectation in
discourse processing and provide direction
for incremental discourse parsing.},
 address = {Taipei, Taiwan},
 author = {Yung, Frances  and  Noji, Hiroshi  and  Matsumoto, Yuji},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {157--162},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Can Discourse Relations be Identified Incrementally?},
 year = {2017}
}

@inproceedings{I17-2028,
 abstract = {Language understanding (LU) and dialogue policy learning are two essential
components in conversational systems. Human-human dialogues are not
well-controlled and often random and unpredictable due to their own goals and
speaking habits. This paper proposes a role-based contextual model to consider
different speaker roles independently based on the various speaking patterns in
the multi-turn dialogues. The experiments on the benchmark dataset show that
the proposed role-based model successfully learns role-specific behavioral
patterns for contextual encoding and then significantly improves language
understanding and dialogue policy learning tasks.},
 address = {Taipei, Taiwan},
 author = {Chi, Ta Chung  and  Chen, Po Chun  and  Su, Shang-Yu  and  Chen, Yun-Nung},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {163--168},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Speaker Role Contextual Modeling for Language Understanding and Dialogue Policy Learning},
 year = {2017}
}

@inproceedings{I17-2029,
 abstract = {Neural conversation systems, typically using sequence-to-sequence (seq2seq)
models, are showing promising progress recently. However, traditional seq2seq
suffer from a severe weakness: during beam search decoding, they tend to rank
universal replies at the top of the candidate list, resulting in the lack of
diversity among candidate replies. Maximum Marginal Relevance (MMR) is a
ranking algorithm that has been widely used for subset selection. In this
paper, we propose the MMR-BS decoding method, which incorporates MMR into the
beam search (BS) process of seq2seq. The MMR-BS method improves the diversity
of generated replies without sacrificing their high relevance with the
user-issued query. Experiments show that our proposed model achieves the best
performance among other comparison methods.},
 address = {Taipei, Taiwan},
 author = {Song, Yiping  and  Tian, Zhiliang  and  Zhao, Dongyan  and  Zhang, Ming  and  Yan, Rui},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {169--174},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Diversifying Neural Conversation Model with Maximal Marginal Relevance},
 year = {2017}
}

@inproceedings{I17-2030,
 abstract = {Generating computer code from natural language descriptions has been a
long-standing problem. Prior work in this domain has restricted itself to
generating code in one shot from a single description. To overcome this
limitation, we propose a system that can engage users in a dialog to clarify
their intent until it has all the information to produce correct code. To
evaluate the efficacy of dialog in code generation, we focus on synthesizing
conditional statements in the form of IFTTT recipes.},
 address = {Taipei, Taiwan},
 author = {Chaurasia, Shobhit  and  Mooney, Raymond J.},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {175--180},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Dialog for Language to Code},
 year = {2017}
}

@inproceedings{I17-2031,
 abstract = {Assessing summaries is a demanding, yet useful task which provides valuable
information on              language competence, especially for second language
learners.
We consider automated scoring of college-level summary writing task in English
as a second language (EL2). We adopt the Reading-for-Understanding (RU)
cognitive framework, extended with the Reading-to-Write (RW) element, and use
analytic scoring with six rubrics covering content and writing quality. We show
that regression models with reference-based and linguistic features
considerably outperform the baselines across all the rubrics. Moreover, we find
interesting correlations between summary features and analytic rubrics, revealing the links between the RU and RW constructs.},
 address = {Taipei, Taiwan},
 author = {Sladoljev Agejev, Tamara  and  \v{S}najder, Jan},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {181--186},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Using Analytic Scoring Rubrics in the Automatic Assessment of College-Level Summary Writing Tasks in L2},
 year = {2017}
}

@inproceedings{I17-2032,
 abstract = {We present in this paper a statistical framework that generates accurate and
fluent product description from product attributes. Specifically, after
extracting templates and learning writing knowledge from attribute-description
parallel data, we use the learned knowledge to decide what to say and how to
say for product description generation. To evaluate accuracy and fluency for
the generated descriptions, in addition to BLEU and Recall, we propose to
measure what to say (in terms of attribute coverage) and to measure how to say
(by attribute-specified generation) separately. Experimental results show that
our framework is effective.},
 address = {Taipei, Taiwan},
 author = {Wang, Jinpeng  and  Hou, Yutai  and  Liu, Jing  and  Cao, Yunbo  and  Lin, Chin-Yew},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {187--192},
 publisher = {Asian Federation of Natural Language Processing},
 title = {A Statistical Framework for Product Description Generation},
 year = {2017}
}

@inproceedings{I17-2033,
 abstract = {An automatic text summarization system can automatically generate a short and
brief summary that contains a main concept of an original document. In this
work, we explore the advantages of simple embedding features in Reinforcement
leaning approach to automatic text summarization tasks. In addition, we propose
a novel deep learning network for estimating Q-values used in Reinforcement
learning. We evaluate our model by using ROUGE scores with DUC 2001, 2002, Wikipedia, ACL-ARC data. Evaluation results show that our model is competitive
with the previous models.},
 address = {Taipei, Taiwan},
 author = {Lee, Gyoung Ho  and  Lee, Kong Joo},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {193--197},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Automatic Text Summarization Using Reinforcement Learning with Embedding Features},
 year = {2017}
}

@inproceedings{I17-2034,
 abstract = {Ideally a metric evaluating an abstract system summary should represent the
extent to which the system-generated summary approximates the semantic
inference conceived by the reader using a human-written reference summary. Most
of the previous approaches relied upon word or syntactic sub-sequence overlap
to evaluate system-generated summaries. Such metrics cannot evaluate the
summary at semantic inference level. Through this work we introduce the metric
of Semantic Similarity for Abstractive Summarization (SSAS), which leverages
natural language inference and paraphrasing techniques to frame a novel
approach to evaluate system summaries at semantic inference level. SSAS is
based upon a weighted composition of quantities representing the level of
agreement, contradiction, independence, paraphrasing, and optionally ROUGE
score between a system-generated and a human-written summary.},
 address = {Taipei, Taiwan},
 author = {Vadapalli, Raghuram  and  J Kurisinkel, Litton  and  Gupta, Manish  and  Varma, Vasudeva},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {198--203},
 publisher = {Asian Federation of Natural Language Processing},
 title = {SSAS: Semantic Similarity for Abstractive Summarization},
 year = {2017}
}

@inproceedings{I17-2035,
 abstract = {Following Gillick and Favre (2009), a lot of work about extractive
summarization has modeled this task by associating two contrary constraints:
one aims at maximizing the coverage of the summary with respect to its
information content while the other represents its size limit. In this context, the notion of redundancy is only implicitly taken into account. In this
article, we extend the framework defined by Gillick and Favre (2009) by
examining how and to what extent integrating semantic sentence similarity into
an update summarization system can improve its results. We show more precisely
the impact of this strategy through evaluations performed on DUC 2007 and TAC
2008 and 2009 datasets.},
 address = {Taipei, Taiwan},
 author = {mnasri, maali  and  de Chalendar, Ga\"{e}l  and  Ferret, Olivier},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {204--209},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Taking into account Inter-sentence Similarity for Update Summarization},
 year = {2017}
}

@inproceedings{I17-2036,
 abstract = {This paper presents an initial study on hyperspherical query likelihood models
(QLMs) for information retrieval (IR). Our motivation is to naturally utilize
pre-trained word embeddings for probabilistic IR. To this end, key idea is to
directly leverage the word embeddings as random variables for directional
probabilistic models based on von Mises-Fisher distributions which are familiar
to cosine distances. The proposed method enables us to theoretically take
semantic similarities between document and target queries into consideration
without introducing heuristic expansion techniques. In addition, this paper
reveals relationships between hyperspherical QLMs and conventional QLMs.
Experiments show document retrieval evaluation results in which a
hyperspherical QLM is compared to conventional QLMs and document distance
metrics using word or document embeddings.},
 address = {Taipei, Taiwan},
 author = {Masumura, Ryo  and  Asami, Taichi  and  Masataki, Hirokazu  and  Sadamitsu, Kugatsu  and  Nishida, Kyosuke  and  Higashinaka, Ryuichiro},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {210--216},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Hyperspherical Query Likelihood Models with Word Embeddings},
 year = {2017}
}

@inproceedings{I17-2037,
 abstract = {Embedding based approaches are shown to be effective for solving simple
Question Answering (QA) problems in recent works. The major drawback of current
approaches is that they look only at the similarity (constraint) between a
question and a head, relation pair. Due to the absence of tail (answer) in the
questions, these models often require paraphrase datasets to obtain adequate
embeddings.
In this paper, we propose a dual constraint model which exploits the embeddings
obtained by Trans* family of algorithms to solve the simple QA problem without
using any additional resources such as paraphrase datasets. The results
obtained prove that the embeddings learned using dual constraints are better
than those with single constraint models having similar architecture.},
 address = {Taipei, Taiwan},
 author = {Kulkarni, Kaustubh  and  Togashi, Riku  and  Maeda, Hideyuki  and  Fujita, Sumio},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {217--221},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Dual Constrained Question Embeddings with Relational Knowledge Bases for Simple Question Answering},
 year = {2017}
}

@inproceedings{I17-2038,
 abstract = {This paper investigates the problem of answering compositional factoid
questions over knowledge bases (KB) under efficiency constraints. The method, called TIPI, (i) decomposes compositional questions, (ii) predicts answer types
for individual sub-questions, (iii) reasons over the compatibility of joint
types, and finally, (iv) formulates compositional SPARQL queries respecting
type constraints. TIPI's answer type predictor is trained using distant
supervision, and exploits lexical, syntactic and embedding-based features to
compute context- and hierarchy-aware candidate answer types for an input
question. Experiments on a recent benchmark show that TIPI results in
state-of-the-art performance under the real-world assumption that only a single
SPARQL query can be executed over the KB, and substantial reduction in the
number of queries in the more general case.},
 address = {Taipei, Taiwan},
 author = {Ziegler, David  and  Abujabal, Abdalghani  and  Saha Roy, Rishiraj  and  Weikum, Gerhard},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {222--227},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Efficiency-aware Answering of Compositional Questions using Answer Type Prediction},
 year = {2017}
}

@inproceedings{I17-2039,
 abstract = {Relation Discovery discovers predicates (relation types) from a text corpus
relying on the co-occurrence of two named entities in the same sentence. This
is a very narrowing constraint: it represents only a small fraction of all
relation mentions in practice.
In this paper we propose a high recall approach for Open IE, which enables
covering up to 16 times more sentences in a large corpus. Comparison against
OpenIE systems shows that our proposed approach achieves 28% improvement over
the highest recall OpenIE system and 6% improvement in precision than the same
system.},
 address = {Taipei, Taiwan},
 author = {Elsahar, Hady  and  Gravier, Christopher  and  Laforest, Frederique},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {228--233},
 publisher = {Asian Federation of Natural Language Processing},
 title = {High Recall Open IE for Relation Discovery},
 year = {2017}
}

@inproceedings{I17-2040,
 abstract = {Focusing on the task of identifying event temporal status, we find that events
directly or indirectly governing the target event in a dependency tree are most
important contexts. Therefore, we extract dependency chains containing context
events and use them as input in neural network models, which consistently
outperform previous models using local context words as input. Visualization
verifies that the dependency chain representation can effectively capture the
context events which are closely related to the target event and play key roles
in predicting event temporal status.},
 address = {Taipei, Taiwan},
 author = {Dai, Zeyu  and  Yao, Wenlin  and  Huang, Ruihong},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {234--239},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Using Context Events in Neural Network Models for Event Temporal Status Identification},
 year = {2017}
}

@inproceedings{I17-2041,
 abstract = {In this paper, we propose a recurrent neural network model for identifying
protein-protein interactions in biomedical literature. Experiments on two
largest public benchmark datasets, AIMed and BioInfer, demonstrate that our
approach significantly surpasses state-of-the-art methods with relative
improvements of 10% and 18%, respectively. Cross-corpus evaluation also
demonstrate that the proposed model remains robust despite using different
training data. These results suggest that RNN can effectively capture semantic
relationships among proteins as well as generalizes over different corpora, without any feature engineering.},
 address = {Taipei, Taiwan},
 author = {Hsieh, Yu-Lun  and  Chang, Yung-Chun  and  Chang, Nai-Wen  and  Hsu, Wen-Lian},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {240--245},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Identifying Protein-protein Interactions in Biomedical Literature using Recurrent Neural Networks with Long Short-Term Memory},
 year = {2017}
}

@inproceedings{I17-2042,
 abstract = {Empathy captures oneâs ability to correlate with and understand othersâ
emotional states and experiences. Messages with empathetic content are
considered as one of the main advantages for joining online health communities
due to their potential to improve peopleâs moods. Unfortunately, to this
date, no computational studies exist that automatically identify empathetic
messages in online health communities. We propose a combination of
Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, and show that the proposed model outperforms each individual model (CNN and
LSTM) as well as several baselines.},
 address = {Taipei, Taiwan},
 author = {Khanpour, Hamed  and  Caragea, Cornelia  and  Biyani, Prakhar},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {246--251},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Identifying Empathetic Messages in Online Health Communities},
 year = {2017}
}

@inproceedings{I17-2043,
 abstract = {Automatic fake news detection is an important, yet very challenging topic.
Traditional methods using lexical features have only very limited success. This
paper proposes a novel method to incorporate speaker profiles into an attention
based LSTM model for fake news detection. Speaker profiles contribute to the
model in two ways. One is to include them in the attention model. The other
includes them as additional input data. By adding speaker profiles such as
party affiliation, speaker title, location and credit history, our model
outperforms the state-of-the-art method by 14.5% in accuracy using a benchmark
fake news detection dataset. This proves that speaker profiles provide valuable
information to validate the credibility of news articles.
Author{4}{Affiliation}},
 address = {Taipei, Taiwan},
 author = {Long, Yunfei  and  Lu, Qin  and  Xiang, Rong  and  Li, Minglei  and  Huang, Chu-Ren},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {252--256},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Fake News Detection Through Multi-Perspective Speaker Profiles},
 year = {2017}
}

@inproceedings{I17-2044,
 abstract = {In this study, we investigated the effectiveness of augmented data for
encoder-decoder-based neural normalization models. Attention based
encoder-decoder models are greatly effective in generating many natural
languages. % such as machine translation or machine summarization.
In general, we have to prepare for a large amount of training data to train an
encoder-decoder model. Unlike machine translation, there are few training data
for text-normalization tasks. In this paper, we propose two methods for
generating augmented data. The experimental results with Japanese dialect
normalization indicate that our methods are effective for an encoder-decoder
model and achieve higher BLEU score than that of baselines. We also
investigated the oracle performance and revealed that there is
sufficient room for improving an encoder-decoder model.
Author{1}{Affiliation}},
 address = {Taipei, Taiwan},
 author = {Saito, Itsumi  and  Suzuki, Jun  and  Nishida, Kyosuke  and  Sadamitsu, Kugatsu  and  Kobashikawa, Satoshi  and  Masumura, Ryo  and  Matsumoto, Yuji  and  Tomita, Junji},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {257--262},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Improving Neural Text Normalization with Data Augmentation at Character- and Morphological Levels},
 year = {2017}
}

@inproceedings{I17-2045,
 abstract = {We propose a hierarchical neural network model for language variety
identification that integrates information from a social network. Recently, language variety identification has enjoyed heightened popularity as an
advanced task of language identification. The proposed model uses additional
texts from a social network to improve language variety identification from two
perspectives. First, they are used to introduce the effects of homophily.
Secondly, they are used as expanded training data for shared layers of the
proposed model. By introducing information from social networks, the model
improved its accuracy by 1.67-5.56. Compared to state-of-the-art baselines, these improved performances are better in English and comparable in Spanish.
Furthermore, we analyzed the cases of Portuguese and Arabic when the model
showed weak performances, and found that the effect of homophily is likely to
be weak due to sparsity and noises compared to languages with the strong
performances.},
 address = {Taipei, Taiwan},
 author = {Miura, Yasuhide  and  Taniguchi, Tomoki  and  Taniguchi, Motoki  and  Misawa, Shotaro  and  Ohkuma, Tomoko},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {263--270},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Using Social Networks to Improve Language Variety Identification with Neural Networks},
 year = {2017}
}

@inproceedings{I17-2046,
 abstract = {Training efficiency is one of the main problems for Neural Machine Translation
(NMT). Deep networks need for very large data as well as many training
iterations to achieve state-of-the-art performance. This results in very high
computation cost, slowing down research and industrialisation. In this paper, we propose to alleviate this problem with several training methods based on
data boosting and bootstrap with no modifications to the neural network.
It imitates the learning process of humans, which typically spend more time
when learning "difficult" concepts than easier ones. We experiment on an
English-French translation task showing accuracy improvements of up to 1.63
BLEU while saving 20% of training time.},
 address = {Taipei, Taiwan},
 author = {Zhang, Dakun  and  Kim, Jungi  and  Crego, Josep  and  Senellart, Jean},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {271--276},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Boosting Neural Machine Translation},
 year = {2017}
}

@inproceedings{I17-2047,
 abstract = {This study reports an attempt to predict the voice of reference using the
information from the input sentences or previous input/output sentences. Our
previous study presented a voice controlling method to generate sentences for
neural machine translation, wherein it was demonstrated that the BLEU score
improved when the voice of generated sentence was controlled relative to that
of the reference. However, it is impractical to use the reference information
because we can- not discern the voice of the correct translation in advance.
Thus, this study presents a voice prediction method for generated sentences for
neural machine translation. While evaluating on Japanese-to-English
translation, we obtain a 0.70-improvement in the BLEU using the predicted
voice.},
 address = {Taipei, Taiwan},
 author = {Yamagishi, Hayahide  and  Kanouchi, Shin  and  Sato, Takayuki  and  Komachi, Mamoru},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {277--282},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Improving Japanese-to-English Neural Machine Translation by Voice Prediction},
 year = {2017}
}

@inproceedings{I17-2048,
 abstract = {We investigate pivot-based translation between related languages in a low
resource, phrase-based SMT setting. We show that a subword-level pivot-based
SMT model using a related pivot language is substantially better than word and
morpheme-level pivot models. It is also highly competitive with the best direct
translation model, which is encouraging as no direct source-target training
corpus is used.
We also show that combining multiple related language pivot models can rival a
direct translation model.
Thus, the use of subwords as translation units coupled with multiple related
pivot languages can compensate for the lack of a direct parallel corpus.},
 address = {Taipei, Taiwan},
 author = {Kunchukuttan, Anoop  and  Shah, Maulik  and  Prakash, Pradyot  and  Bhattacharyya, Pushpak},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {283--289},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Utilizing Lexical Similarity between Related, Low-resource Languages for Pivot-based SMT},
 year = {2017}
}

@inproceedings{I17-2049,
 abstract = {In this paper, we propose a neural machine translation (NMT) with a key-value
attention mechanism on the source-side encoder. The key-value attention
mechanism separates the source-side content vector into two types of memory
known as the key and the value. The key is used for calculating the attention
distribution, and the value is used for encoding the context representation.
Experiments on three different tasks indicate that our model outperforms an NMT
model with a conventional attention mechanism. Furthermore, we perform
experiments with a conventional NMT framework, in which a part of the initial
value of a weight matrix is set to zero so that the matrix is as the same
initial-state as the key-value attention mechanism. As a result, we obtain
comparable results with the key-value attention mechanism without changing the
network structure.},
 address = {Taipei, Taiwan},
 author = {Mino, Hideya  and  Utiyama, Masao  and  Sumita, Eiichiro  and  Tokunaga, Takenobu},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {290--295},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Key-value Attention Mechanism for Neural Machine Translation},
 year = {2017}
}

@inproceedings{I17-2050,
 abstract = {We present a simple method to improve neural translation of a low-resource
language pair using parallel data from a related, also low-resource, language
pair.
The method is based on the transfer method of Zoph et al., but whereas their
method ignores any source vocabulary overlap, ours exploits it. First, we split
words using Byte Pair Encoding (BPE) to increase vocabulary overlap. Then, we
train a model on the first language pair and transfer its parameters, including
its source word embeddings, to another model and continue training on the
second language pair. Our experiments show that transfer learning helps
word-based translation only slightly, but when used on top of a much stronger
BPE baseline, it yields larger improvements of up to 4.3 BLEU.},
 address = {Taipei, Taiwan},
 author = {Nguyen, Toan Q.  and  Chiang, David},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {296--301},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation},
 year = {2017}
}

@inproceedings{I17-2051,
 abstract = {Neural machine translation decoders are usually conditional language models to
sequentially generate words
for target sentences.
This approach is limited to find the best word composition and requires help of
explicit methods as beam search.
To help learning correct compositional mechanisms in NMTs, we propose concept equalization using direct mapping distributed
representations of source and target sentences.
In a translation experiment from English to French, the concept equalization
significantly improved translation quality by 3.00 BLEU points
compared to a state-of-the-art NMT model.},
 address = {Taipei, Taiwan},
 author = {kim, kangil  and  Shin, Jong-Hun  and  Na, Seung-Hoon  and  Jung, SangKeun},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {302--307},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Concept Equalization to Guide Correct Training of Neural Machine Translation},
 year = {2017}
}

@inproceedings{I17-2052,
 abstract = {We present PubMed 200k RCT, a new dataset based on PubMed for sequential
sentence classification. The dataset consists of approximately 200,000
abstracts of randomized controlled trials, totaling 2.3 million sentences. Each
sentence of each abstract is labeled with their role in the abstract using one
of the following classes: background, objective, method, result, or conclusion.
The purpose of releasing this dataset is twofold. First, the majority of
datasets for sequential short-text classification (i.e., classification of
short texts that appear in sequences) are small: we hope that releasing a new
large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently
skim through the literature. Automatically classifying each sentence in an
abstract would help researchers read abstracts more efficiently, especially in
fields where abstracts may be long, such as the medical field.},
 address = {Taipei, Taiwan},
 author = {Dernoncourt, Franck  and  Lee, Ji Young},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {308--313},
 publisher = {Asian Federation of Natural Language Processing},
 title = {PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts},
 year = {2017}
}

@inproceedings{I17-2053,
 abstract = {Automated documentation of programming source code and automated code
generation from natural language are challenging tasks of both practical and
scientific interest.
Progress in these areas has been limited by the low availability of parallel
corpora of code and natural language descriptions, which tend to be small and
constrained to specific domains.
In this work we introduce a large and diverse parallel corpus of a hundred
thousands Python functions with their documentation strings ("docstrings")
generated by scraping open source repositories on GitHub.
We describe baseline results for the code documentation and code generation
tasks obtained by neural machine translation.
We also experiment with data augmentation techniques to further increase the
amount of training data.
We release our datasets and processing scripts in order to stimulate research
in these areas.},
 address = {Taipei, Taiwan},
 author = {Miceli Barone, Antonio Valerio  and  Sennrich, Rico},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {314--319},
 publisher = {Asian Federation of Natural Language Processing},
 title = {A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation},
 year = {2017}
}

@inproceedings{I17-2054,
 abstract = {Corpus is a valuable resource for information retrieval and data-driven
natural language processing systems,especially for spoken dialogue research in
specific domains. However,there is little non-English corpora, particular for
ones in Chinese. Spoken by the nation with the largest population
in  the  world,  Chinese  become increasingly prevalent and popular
among millions of people worldwide. In this paper, we build a large-scale
and high-quality Chinese corpus, called CSDC (Chinese Spoken Dialogue Corpus).
It contains five domains and more than 140 thousand dialogues in all. Each
sentence in this corpus is annotated with slot information additionally
compared to other corpora. To our best knowledge, this is the largest Chinese
spoken dialogue corpus, as well as the first one with slot information. With
this corpus, we proposed a method and did a well-designed experiment. The
indicative result is reported at last.},
 address = {Taipei, Taiwan},
 author = {Li, Changliang  and  Wang, Xiuying},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {320--324},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Building Large Chinese Corpus for Spoken Dialogue Research in Specific Domains},
 year = {2017}
}

@inproceedings{I17-2055,
 abstract = {We present the first study that evaluates both speaker and listener
identification for direct speech in literary texts. Our approach consists of
two steps: identification of speakers and listeners near the quotes, and
dialogue chain segmentation. Evaluation results show that this approach
outperforms a rule-based approach that is state-of-the-art on a corpus of
literary texts.},
 address = {Taipei, Taiwan},
 author = {Yeung, Chak Yan  and  Lee, John},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {325--329},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Identifying Speakers and Listeners of Quoted Speech in Literary Works},
 year = {2017}
}

@inproceedings{I17-2056,
 abstract = {The psycholinguistic properties of words, namely, word familiarity, age of
acquisition, concreteness, and imagery, have been reported to be effective for
educational natural language-processing tasks. Previous studies on predicting
the values of these properties rely on language-dependent features. This paper
is the first to propose a practical language-independent method for predicting
such values by using only a large raw corpus in a language. Through
experiments, our method successfully predicted the values of these properties
in two languages. The results for English were competitive with the reported
accuracy achieved using features specific to English.},
 address = {Taipei, Taiwan},
 author = {Ehara, Yo},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {330--336},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Language-Independent Prediction of Psycholinguistic Properties of Words},
 year = {2017}
}

@inproceedings{I17-2057,
 abstract = {It is very costly and time consuming to find new biomarkers for specific
diseases in clinical laboratories. In this study, to find new biomarkers most
closely related to Chronic Obstructive Pulmonary Disease (COPD), which is
widely known as respiratory disease, biomarkers known to be associated with
respiratory diseases and COPD itself were converted into word embedding. And
their similarities were measured. We used Word2Vec, Canonical Correlation
Analysis (CCA), and Global Vector (GloVe) for word embedding. In order to
replace the clinical evaluation, the titles and abstracts of papers retrieved
from Google Scholars were analyzed and quantified to estimate the performance
of the word em-bedding models.},
 address = {Taipei, Taiwan},
 author = {Yoon, Byeong-Hun  and  Kim, Yu-Seop},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {337--342},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Correlation Analysis of Chronic Obstructive Pulmonary Disease (COPD) and its Biomarkers Using the Word Embeddings},
 year = {2017}
}

@inproceedings{I17-2058,
 abstract = {In grammatical error correction (GEC), automatically evaluating system outputs
requires gold-standard references, which must be created manually and thus tend
to be both expensive and limited in coverage. To address this problem, a
reference-less approach has recently emerged; however, previous reference-less
metrics that only consider the criterion of grammaticality, have not worked as
well as reference-based metrics. This study explores the potential of extending
a prior grammaticality-based method to establish a reference-less evaluation
method for GEC systems. Further, we empirically show that a reference-less
metric that combines fluency and meaning preservation with grammaticality
provides a better estimate of manual scores than that of commonly used
reference-based metrics. To our knowledge, this is the first study that
provides empirical evidence that a reference-less metric can replace
reference-based metrics in evaluating GEC systems.},
 address = {Taipei, Taiwan},
 author = {Asano, Hiroki  and  Mizumoto, Tomoya  and  Inui, Kentaro},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {343--348},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Reference-based Metrics can be Replaced with Reference-less Metrics in Evaluating Grammatical Error Correction Systems},
 year = {2017}
}

@inproceedings{I17-2059,
 abstract = {Automatic analysis of curriculum vitae (CVs) of applicants is of tremendous
importance in recruitment scenarios. The semi-structuredness of CVs, however, makes CV processing a challenging task. We propose a solution towards
transforming CVs to follow a unified structure, thereby, paving ways for
smoother CV analysis. The problem of restructuring is posed as a section
relabeling problem, where each section of a given CV gets reassigned to a
predefined label. Our relabeling method relies on semantic relatedness computed
between section header, content and labels, based on phrase-embeddings learned
from a large pool of CVs. We follow different heuristics to measure semantic
relatedness. Our best heuristic achieves an F-score of 93.17% on a test
dataset with gold-standard labels obtained using manual annotation.},
 address = {Taipei, Taiwan},
 author = {Garg, Shweta  and  Singh, Sudhanshu S  and  Mishra, Abhijit  and  Dey, Kuntal},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {349--354},
 publisher = {Asian Federation of Natural Language Processing},
 title = {CVBed: Structuring CVs usingWord Embeddings},
 year = {2017}
}

@inproceedings{I17-2060,
 abstract = {In this work we study the challenging task of automatically constructing essays
for Chinese college entrance examination where the topic is specified in
advance. We explore a sentence extraction framework based on diversified
lexical
chains to capture coherence and richness. Experimental analysis shows the
effectiveness of our approach and reveals the importance of information
richness in essay writing.},
 address = {Taipei, Taiwan},
 author = {Li, Liunian  and  Wan, Xiaojun  and  Yao, Jin-ge  and  Yan, Siming},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {355--360},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Leveraging Diverse Lexical Chains to Construct Essays for Chinese College Entrance Examination},
 year = {2017}
}

@inproceedings{I17-2061,
 abstract = {While language conveys meaning largely symbolically, actual communication acts
typically contain iconic elements as well: People gesture while they speak, or
may even draw sketches while explaining something.
Image retrieval prima facie seems like a task that could profit from combined
symbolic and iconic reference, but it is typically set up to work either from
language only, or via (iconic) sketches with no verbal contribution. Using a
model of grounded language semantics and a model of sketch-to-image mapping, we
show that adding even very reduced iconic information to a verbal image
description improves recall. Verbal descriptions paired with fully detailed
sketches still perform better than these sketches alone. We see these results
as supporting the assumption that natural user interfaces should respond to
multimodal input, where possible, rather than just language alone.},
 address = {Taipei, Taiwan},
 author = {Han, Ting  and  Schlangen, David},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {361--365},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Draw and Tell: Multimodal Descriptions Outperform Verbal- or Sketch-Only Descriptions in an Image Retrieval Task},
 year = {2017}
}

@inproceedings{I17-2062,
 abstract = {We propose a neural encoder-decoder model with reinforcement learning (NRL) for
grammatical error correction (GEC). Unlike conventional maximum likelihood
estimation (MLE), the model directly optimizes towards an objective that
considers a sentence-level, task-specific evaluation metric, avoiding the
exposure bias issue in MLE. We demonstrate that NRL outperforms MLE both in
human and automated evaluation metrics, achieving the state-of- the-art on a
fluency-oriented GEC corpus.},
 address = {Taipei, Taiwan},
 author = {Sakaguchi, Keisuke  and  Post, Matt  and  Van Durme, Benjamin},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {366--372},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Grammatical Error Correction with Neural Reinforcement Learning},
 year = {2017}
}

@inproceedings{I17-2063,
 abstract = {This paper describes a coreference resolution system for math problem text.
Case frame dictionaries and a math taxonomy are utilized for supplying domain
knowledge. The system deals with various anaphoric phenomena beyond
well-studied entity coreferences.},
 address = {Taipei, Taiwan},
 author = {Ito, Takumi  and  Matsuzaki, Takuya  and  Sato, Satoshi},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {373--377},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Coreference Resolution on Math Problem Text in Japanese},
 year = {2017}
}

@inproceedings{I17-2064,
 abstract = {We propose a novel method that exploits visual information of ideograms and
logograms in analyzing Japanese review documents.  Our method first converts
font images of Japanese characters into character embeddings using
convolutional neural networks. It then constructs document embeddings from the
character embeddings based on Hierarchical Attention Networks, which represent
the documents based on attention mechanisms from a character level to a
sentence level. The document embeddings are finally used to predict the labels
of documents. Our method provides a way to exploit visual features of
characters in languages with ideograms and logograms. In the experiments, our
method achieved an accuracy comparable to a character embedding-based model
while our method has much fewer parameters since it does not need to keep
embeddings of thousands of characters.},
 address = {Taipei, Taiwan},
 author = {Toyama, Yota  and  Miwa, Makoto  and  Sasaki, Yutaka},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {378--382},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Utilizing Visual Forms of Japanese Characters for Neural Review Classification},
 year = {2017}
}

@inproceedings{I17-2065,
 abstract = {We show how to adapt bilingual word embeddings (BWE's) to bootstrap a
cross-lingual name-entity recognition (NER) system in a language with no
labeled data. We assume a setting where we are given a comparable corpus with
NER labels for the source language only; our goal is to build a NER model for
the target language. The proposed multi-task model jointly trains bilingual
word embeddings while optimizing a NER objective. This creates word embeddings
that are both shared between languages and fine-tuned for the NER task.},
 address = {Taipei, Taiwan},
 author = {Wang, Dingquan  and  Peng, Nanyun  and  Duh, Kevin},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {383--388},
 publisher = {Asian Federation of Natural Language Processing},
 title = {A Multi-task Learning Approach to Adapting Bilingual Word Embeddings for Cross-lingual Named Entity Recognition},
 year = {2017}
}

@inproceedings{I17-2066,
 abstract = {In dialogue systems, conveying understanding results of user utterances is
important because it enables users to feel understood by the system. However, it is not clear what types of understanding results should be conveyed to
users; some utterances may be offensive and some may be too commonsensical. In
this paper, we explored the effect of conveying understanding results of user
utterances in a chat-oriented dialogue system by an experiment using human
subjects. As a result, we found that only certain types of understanding
results, such as those related to a user's permanent state, are effective to
improve user satisfaction. This paper clarifies the types of understanding
results that can be safely uttered by a system.},
 address = {Taipei, Taiwan},
 author = {Mitsuda, Koh  and  Higashinaka, Ryuichiro  and  Tomita, Junji},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {389--394},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Investigating the Effect of Conveying Understanding Results in Chat-Oriented Dialogue Systems},
 year = {2017}
}

@inproceedings{I17-2067,
 abstract = {Contrastive opinion mining is essential in identifying, extracting and
organising opinions from user generated texts. Most existing studies separate
input data into respective collections. In addition, the relationships between
the topics  extracted and the sentences in the corpus which express the topics
are opaque, hindering our understanding of the opinions expressed in the
corpus. We propose a novel unified latent variable model (contraLDA) which
addresses the above matters. Experimental results show the effectiveness of our
model in mining contrasted opinions, outperforming our baselines.},
 address = {Taipei, Taiwan},
 author = {Ibeke, Ebuka  and  Lin, Chenghua  and  Wyner, Adam  and  Barawi, Mohamad Hardyman},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {395--400},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Extracting and Understanding Contrastive Opinion through Topic Relevant Sentences},
 year = {2017}
}

@inproceedings{I17-2068,
 abstract = {Complex word identification (CWI) is an important task in text accessibility.
However, due to the scarcity of CWI datasets, previous studies have only
addressed this problem on Wikipedia sentences and have solely taken into
account the needs of non-native English speakers. We collect a new CWI dataset
(CWIG3G2) covering three text genres News, WikiNews, and Wikipedia) annotated
by both native and non-native English speakers. Unlike previous datasets, we
cover single words, as well as complex phrases, and present them for judgment
in a paragraph context. We present the first study on cross-genre and
cross-group CWI, showing measurable influences in native language and genre
types.},
 address = {Taipei, Taiwan},
 author = {Yimam, Seid Muhie  and  \v{S}tajner, Sanja  and  Riedl, Martin  and  Biemann, Chris},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {401--407},
 publisher = {Asian Federation of Natural Language Processing},
 title = {CWIG3G2 - Complex Word Identification Task across Three Text Genres and Two User Groups},
 year = {2017}
}

@inproceedings{I17-2069,
 abstract = {We propose a novel, data-driven, and stylistically consistent dialog response
generation system.
To create a user-friendly system, it is crucial to make generated responses not
only appropriate but also stylistically consistent. For leaning both the
properties effectively, our proposed framework has two training stages inspired
by transfer learning.
First, we train the model to generate appropriate responses, and then we ensure
that the responses have a specific style.
Experimental results demonstrate that the proposed method produces
stylistically consistent responses while maintaining the appropriateness of the
responses learned in a general domain.},
 address = {Taipei, Taiwan},
 author = {Akama, Reina  and  Inada, Kazuaki  and  Inoue, Naoya  and  Kobayashi, Sosuke  and  Inui, Kentaro},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {408--412},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Generating Stylistically Consistent Dialog Responses with Transfer Learning},
 year = {2017}
}

@inproceedings{I17-2070,
 abstract = {We describe a data-driven approach for automatically explaining new, non-standard English expressions in a given sentence, building on a large
dataset that includes 15 years of crowdsourced examples from
UrbanDictionary.com. Unlike prior studies that focus on matching keywords from
a slang dictionary, we investigate the possibility of learning a neural
sequence-to-sequence model that generates explanations of unseen non-standard
English expressions given context. We propose a dual encoder approach---a
word-level encoder learns the representation of context, and a second
character-level encoder to learn the hidden representation of the target
non-standard expression. Our model can produce reasonable definitions of new
non-standard English expressions given their context with certain confidence.},
 address = {Taipei, Taiwan},
 author = {Ni, Ke  and  Wang, William Yang},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {413--417},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Learning to Explain Non-Standard English Words and Phrases},
 year = {2017}
}

@inproceedings{I17-2071,
 abstract = {We propose a submodular function-based summarization system which integrates
three important measures namely importance, coverage, and non-redundancy to
detect the important sentences for the summary. We design monotone and
submodular functions which allow us to apply an efficient and scalable greedy
algorithm to obtain informative and well-covered summaries. In addition, we
integrate two abstraction-based methods namely sentence compression and merging
for generating an abstractive sentence set. We design our summarization models
for both generic and query-focused summarization. Experimental results on
DUC-2004 and DUC-2007 datasets show that our generic and query-focused
summarizers have outperformed the state-of-the-art summarization systems in
terms of ROUGE-1 and ROUGE-2 recall and F-measure.},
 address = {Taipei, Taiwan},
 author = {Chali, Yllias  and  Tanvee, Moin  and  Nayeem, Mir Tafseer},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {418--424},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Towards Abstractive Multi-Document Summarization Using Submodular Function-Based Framework, Sentence Compression and Merging},
 year = {2017}
}

@inproceedings{I17-2072,
 abstract = {Relations are expressed in many domains such as newswire, weblogs and phone
conversations. Trained on a source domain, a relation extractor's performance
degrades when applied to target domains other than the source. A common yet
labor-intensive method for domain adaptation is to construct a
target-domain-specific labeled dataset for adapting the extractor. In response, we present an unsupervised domain adaptation method which only requires labels
from the source domain. Our method is a joint model consisting of a CNN-based
relation classifier and a domain-adversarial classifier. The two components are
optimized jointly to learn a domain-independent representation for prediction
on the target domain. Our model outperforms the state-of-the-art on all three
test domains of ACE 2005.},
 address = {Taipei, Taiwan},
 author = {Fu, Lisheng  and  Nguyen, Thien Huu  and  Min, Bonan  and  Grishman, Ralph},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {425--429},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Domain Adaptation for Relation Extraction with Domain Adversarial Neural Network},
 year = {2017}
}

@inproceedings{I17-2073,
 abstract = {We explore the application of a Deep Structured Similarity Model (DSSM) to
ranking in lexical simplification. Our results show that the DSSM can
effectively capture fine-grained features to perform semantic matching when
ranking substitution candidates, outperforming the state- of-the-art on two
standard datasets used for the task.},
 address = {Taipei, Taiwan},
 author = {Pereira, Lis  and  Liu, Xiaodong  and  Lee, John},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {430--435},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Lexical Simplification with the Deep Structured Similarity Model},
 year = {2017}
}

@inproceedings{I17-2074,
 abstract = {This paper explores the idea of robot editors, automated proofreaders that
enable journalists to improve the quality of their articles.
We propose a novel neural model of multi-task learning that both generates
proofread sentences and predicts the editing operations required to rewrite the
source sentences and create the proofread ones.
The model is trained using logs of the revisions made professional editors
revising draft newspaper articles written by journalists.
Experiments demonstrate the effectiveness of our multi-task learning approach
and the potential value of using revision logs for this task.},
 address = {Taipei, Taiwan},
 author = {Hitomi, Yuta  and  Tamori, Hideaki  and  Okazaki, Naoaki  and  Inui, Kentaro},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {436--441},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Proofread Sentence Generation as Multi-Task Learning with Editing Operation Prediction},
 year = {2017}
}

@inproceedings{I17-2075,
 abstract = {The automation of tasks in community question answering (cQA) is dominated by
machine learning approaches, whose performance is often limited by the number
of training examples. Starting from a neural sequence learning approach with
attention, we explore the impact of two data augmentation techniques on
question ranking performance: a method that swaps reference questions with
their paraphrases, and training on examples automatically selected from
external datasets. Both methods are shown to lead to substantial gains in
accuracy over a strong baseline. Further improvements are obtained by changing
the model architecture to mirror the structure seen in the data.},
 address = {Taipei, Taiwan},
 author = {Chen, Charles  and  Bunescu, Razvan},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {442--447},
 publisher = {Asian Federation of Natural Language Processing},
 title = {An Exploration of Data Augmentation and RNN Architectures for Question Ranking in Community Question Answering},
 year = {2017}
}

@inproceedings{I17-2076,
 abstract = {What can you do with multiple noisy versions of the same text? We present a
method which generates a single consensus between multi-parallel corpora. By
maximizing a function of linguistic features between word pairs, we jointly
learn a single corpus-wide multiway alignment: a consensus between 27 versions
of the English Bible. We additionally produce English paraphrases, word-level
distributions of tags, and consensus dependency parses. Our method is language
independent and applicable to any multi-parallel corpora. Given the Bibleâs
unique role as alignable bitext for over 800 of the worldâs languages, this
consensus alignment and resulting resources offer value for multilingual
annotation projection, and also shed potential insights into the Bible itself.},
 address = {Taipei, Taiwan},
 author = {Xia, Patrick  and  Yarowsky, David},
 booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 month = {November},
 pages = {448--453},
 publisher = {Asian Federation of Natural Language Processing},
 title = {Deriving Consensus for Multi-Parallel Corpora: an English Bible Study},
 year = {2017}
}

